# 实现、应用程序和工具

在没有任何实际应用的情况下学习算法仍然是纯粹的学术追求。在本章中，我们将探讨塑造我们世界的数据结构和算法。

这个时代的金块之一是丰富的数据。电子邮件、电话号码、文本和图像文档包含大量数据。在这些数据中发现了使数据变得更加重要的有价值的信息。但要从原始数据中提取这些信息，我们必须使用专门用于此任务的数据结构、流程和算法。

机器学习使用大量的算法来分析和预测某些变量的发生。在纯数字的基础上分析数据仍然会在原始数据中留下许多潜在信息。因此，直观地呈现数据也能让人理解并获得有价值的见解。

在本章结束时，您应该能够执行以下操作：

*   精确地修剪和显示数据
*   使用有监督和无监督学习算法进行预测
*   直观地表示数据以获得更多的洞察力

# 行业工具

为了继续本章，您需要安装以下软件包。这些包将用于预处理和可视化地表示正在处理的数据。其中一些软件包还包含编写良好且完善的算法，可以对我们的数据进行操作。

这些模块最好安装在虚拟环境中，如`pip`：

```
% pip install numpy
% pip install scikit-learn
% pip install matplotlib
% pip install pandas
% pip install textblob  
```

这些软件包可能需要先安装其他特定于平台的模块。注意并安装所有依赖项：

*   **Numpy**：一个库，具有对 n 维数组和矩阵进行操作的函数。
*   **Scikit 学习**：高度先进的机器学习模块。它包含大量用于分类、回归和聚类等的算法。
*   **Matplotlib**：这是一个绘图库，利用 NumPy 绘制各种图表，包括直线图、直方图、散点图，甚至 3D 图。
*   **熊猫**：该库处理数据操作和分析。

# 数据预处理

从现实世界收集数据充满了巨大的挑战。收集到的原始数据有很多问题，以至于我们需要采取一些方法对数据进行清理，使其适合用于进一步的研究。

# 为什么要处理原始数据？

从现场收集的原始数据存在人为错误。数据输入是收集数据时的主要错误源。甚至收集数据的技术方法也不能幸免。不准确的设备读数、有故障的小工具以及环境因素的变化都会在收集数据时引入较大的误差幅度。

收集的数据也可能与随时间收集的其他记录不一致。重复条目和不完整记录的存在保证了我们以这样一种方式来处理数据，以揭示隐藏和埋藏的宝藏。原始数据也可能隐藏在无关数据的海洋中。

为了清理数据，我们可以完全丢弃不相关的数据，即噪声。缺少部分或属性的数据可以替换为合理的估计。此外，如果原始数据存在不一致性，则有必要对其进行检测和纠正。

让我们探讨如何使用 NumPy 和 pandas 进行数据预处理技术。

# 缺失数据

数据收集是单调乏味的，因此，一旦收集到数据，就不应轻易丢弃。数据集缺少字段或属性并不意味着它没有用处。有几种方法可以用来填充不存在的部分。其中一种方法是使用全局常量、使用数据集中的平均值或手动提供数据。选择是基于数据将用于什么的上下文和敏感性。

以以下数据为例：

```
    import numpy as np 
    data = pandas.DataFrame([ 
        [4., 45., 984.], 
        [np.NAN, np.NAN, 5.], 
        [94., 23., 55.], 
    ]) 
```

我们可以看到，数据元素`data[1][0]`和`data[1][1]`的值为`np.NAN`，表示它们没有值。如果`np.NAN`值在给定数据集中是不需要的，则可以将其设置为某个常量。

让我们将值`np.NAN`设置为 0.1 的数据元素：

```
    print(data.fillna(0.1)) 
```

数据的新状态如下所示：

```
0     1      2
0   4.0  45.0  984.0
1   0.1   0.1    5.0
2  94.0  23.0   55.0
```

为了应用平均值，我们执行以下操作：

```
    print(data.fillna(data.mean())) 
```

计算每列的平均值，并将其插入具有`np.NAN`值的数据区域：

```
0     1      2
0   4.0  45.0  984.0
1  49.0  34.0    5.0
2  94.0  23.0   55.0
```

对于第一列`0`列，通过`(4 + 94)/2`获得平均值。然后将得到的`49.0`存储在`data[1][0]`。对`1`列和`2`列执行类似操作

# 特征缩放

数据框中的列称为其特征。这些行称为记录或观察。现在检查以下数据矩阵。这些数据将在小节中引用，因此请注意：

```
[[  58\.    1\.   43.]
 [  10\.  200\.   65.]
 [  20\.   75\.    7.]]
```

特征 1 的数据为`58`、`10`、`20`，其值介于`10`和`58`之间。对于特征 2，数据位于`1`和`200`之间。如果我们将这些数据提供给任何机器学习算法，就会产生不一致的结果。理想情况下，我们需要将数据缩放到一定范围，以获得一致的结果。

再次，仔细检查发现每个特征（或列）位于不同的平均值附近。因此，我们要做的是围绕类似的方式对齐这些特性。

特征缩放的一个好处是它促进了机器学习的学习部分。

`scikit`模块有大量的缩放算法，我们将应用于我们的数据。

# 最小最大标量

标准化的最小-最大标量形式使用平均值和标准偏差将所有数据装箱到介于某个最小值和最大值之间的范围内。在大多数情况下，范围设置在 0 和 1 之间。在其他情况下，可以应用其他范围，但 0 到 1 范围仍然是默认值：

```
    scaled_values = preprocessing.MinMaxScaler(feature_range=(0,1)) 
    results = scaled_values.fit(data).transform(data) 
    print(results) 
```

`MinMaxScaler`类的一个实例被创建为范围为`(0,1)`并传递给变量`scaled_values`。调用`fit`函数进行必要的计算，这些计算将在内部用于更改数据集。`transform`函数影响数据集的实际操作，将值返回到`results`：

```
[[ 1\.          0\.          0.62068966]
 [ 0\.          1\.          1\.        ]
 [ 0.20833333  0.3718593   0\.        ]]
```

我们可以从前面的输出中看到，所有的数据都是标准化的，并且位于 0 和 1 之间。这种输出现在可以提供给机器学习算法。

# 标准标量

初始数据集或表格中各特征的平均值分别为 29.3、92 和 38。为了使所有数据具有相似的平均值，即数据的零平均值和单位方差，我们将采用标准标量算法：

```
    stand_scalar =  preprocessing.StandardScaler().fit(data) 
    results = stand_scalar.transform(data) 
    print(results)
```

`data`传递给实例化`StandardScaler`类返回的对象的`fit`方法。`transform`方法作用于数据中的数据元素，并将输出返回结果：

```
[[ 1.38637564 -1.10805456  0.19519899]
 [-0.93499753  1.31505377  1.11542277]
 [-0.45137812 -0.2069992  -1.31062176]]
```

通过检查结果，我们发现我们的所有功能现在都是均匀分布的。

# 二值化数据

为了对给定的特性集进行二值化，我们使用了一个阈值。如果给定数据集中的任何值大于阈值，则该值将替换为 1。如果该值小于阈值 1，我们将替换它：

```
    results = preprocessing.Binarizer(50.0).fit(data).transform(data) 
    print(results) 
```

`Binarizer`的一个实例是使用参数 50.0 创建的。50.0 是将在二值化算法中使用的阈值：

```
[[ 1\.  0\.  0.]
 [ 0\.  1\.  1.]
 [ 0\.  1\.  0.]] 
```

数据中小于 50 的所有值将以 0 代替。反之亦然。

# 机器学习

机器学习是人工智能的一个分支。我们知道，我们永远无法真正创造出真正“思考”的机器，但我们可以为机器提供足够的数据和模型，通过这些数据和模型可以做出正确的判断。机器学习的重点是创建能够继续决策过程的自治系统，几乎不需要人工干预。

为了教机器，我们需要从现实世界中提取数据。例如，为了改变哪些电子邮件构成垃圾邮件，哪些不构成垃圾邮件，我们需要向机器提供每种邮件的样本。在获得这些数据之后，我们必须通过模型（算法）来运行数据，这些模型（算法）将使用概率和统计学从数据中挖掘模式和结构。如果这项工作做得好，算法本身就能够分析电子邮件并对其进行正确分类。对电子邮件进行分类只是机器经过“培训”后可以做什么的一个例子。

# 机器学习的类型

机器学习有三大类，如下所示：

*   **监督学习**：这里，一个算法由一组输入及其相应的输出组成。然后，算法必须计算出不熟悉输入的输出。此类算法的示例包括朴素贝叶斯、线性回归和决策树算法。
*   **无监督学习**：无监督学习算法不使用一组输入和输出变量之间存在的关系，只使用输入来挖掘数据中的组、模式和簇。此类算法的示例包括层次聚类和 k-均值聚类。
*   **强化学习**：在这种学习中，计算机与环境进行动态交互，以提高其性能。

# 你好分类器

为了让编程之神保佑我们理解机器学习，我们从一个文本分类器的 hello world 示例开始。这是对机器学习的温和介绍。

这个例子将预测一个给定的文本是否具有消极或积极的内涵。在这之前，我们需要用一些数据来训练我们的算法（模型）。

朴素贝叶斯模型适用于文本分类。基于朴素贝叶斯模型的算法通常速度快，结果准确。整个模型基于特征相互独立的假设。要准确预测降雨的发生，需要考虑三个条件。这些是风速、温度和空气中的湿度。事实上，这些因素之间确实存在相互影响，以判断降雨的可能性。但是朴素贝叶斯的抽象是假设这些特征在任何方面都是无关的，因此独立地影响降雨的机会。朴素贝叶斯在预测未知数据集的类别时很有用，我们很快就会看到。

现在回到我们的 hello 分类器。在我们训练模式后，其预测将分为积极或消极两类：

```
    from textblob.classifiers import NaiveBayesClassifier 
    train = [ 
        ('I love this sandwich.', 'pos'), 
        ('This is an amazing shop!', 'pos'), 
        ('We feel very good about these beers.', 'pos'), 
        ('That is my best sword.', 'pos'), 
        ('This is an awesome post', 'pos'), 
        ('I do not like this cafe', 'neg'), 
        ('I am tired of this bed.', 'neg'), 
        ("I can't deal with this", 'neg'), 
        ('She is my sworn enemy!', 'neg'), 
        ('I never had a caring mom.', 'neg') 
    ] 
```

首先，我们将从`textblob`包中导入`NaiveBayesClassifier`类。该分类器非常容易使用，并且基于贝叶斯定理。

`train`变量由元组组成，每个元组保存实际的训练数据。每个元组包含句子及其关联的组。

现在，为了训练我们的模型，我们将通过将训练传递给`NaiveBayesClassifier`对象来实例化它：

```
    cl = NaiveBayesClassifier(train) 
```

更新后的朴素贝叶斯模型`cl`将预测未知句子所属的类别。到目前为止，我们的模型只知道一个短语可以属于两个类别，`neg`和`pos`。

以下代码使用我们的模型运行以下测试：

```
    print(cl.classify("I just love breakfast")) 
    print(cl.classify("Yesterday was Sunday")) 
    print(cl.classify("Why can't he pay my bills")) 
    print(cl.classify("They want to kill the president of Bantu")) 
```

我们的测试结果如下：

```
pos 
pos 
neg 
neg 
```

我们可以看到，该算法在将输入短语很好地分类方面取得了一定程度的成功。

这个人为设计的例子过于简单，但它确实表明，如果给定适当的数据量和合适的算法或模型，机器就有可能在没有任何人工帮助的情况下执行任务。

专业类`NaiveBayesClassifier`也在后台为我们做了一些繁重的工作，因此我们无法理解算法得出各种预测的内在原因。我们的下一个示例将使用`scikit`模块预测短语可能属于的类别。

# 一个监督学习的例子

假设我们有一组要分类的帖子。与监督学习一样，我们需要首先训练模型，以便准确预测未知职位的类别。

# 收集数据

`scikit`模块附带了大量样本数据，我们将用于训练我们的模型。在本例中，我们将使用新闻组帖子。要加载帖子，我们将使用以下代码行：

```
    from sklearn.datasets import fetch_20newsgroups 
    training_data = fetch_20newsgroups(subset='train',     
        categories=categories, shuffle=True, random_state=42) 
```

训练模型后，预测结果必须属于以下类别之一：

```
    categories = ['alt.atheism', 
                  'soc.religion.christian','comp.graphics', 'sci.med'] 
```

我们将用作培训数据的记录数量通过以下方式获得：

```
    print(len(training_data)) 
```

机器学习算法不能很好地与文本属性相结合，因此每个帖子所属的类别都以数字表示：

```
    print(set(training_data.target)) 
```

类别具有整数值，我们可以使用`print(training_data.target_names[0])`映射回类别本身。

这里，0 是从`set(training_data.target)`中选取的数字随机索引。

既然已经获得了训练数据，我们就必须将数据提供给机器学习算法。单词袋模型将分解训练数据，以便为学习算法或模型做好准备。

# 字里行间

单词包是一种用于表示文本数据的模型，它不考虑单词的顺序，而是使用单词计数将单词分割成区域。

用下面的句子：

```
    sentence_1 = "As fit as a fiddle"
    sentence_2 = "As you like it"
```

单词袋使我们能够将文本分解为由矩阵表示的数字特征向量。

要将我们的两句话简化为单词袋模型，我们需要获得所有单词的唯一列表：

```
    set((sentence_1 + sentence_2).split(" ")) 
```

这一组将成为我们在矩阵中的列。矩阵中的行表示培训中使用的文档。行和列的交点将存储该单词在文档中出现的次数。以我们的两句话为例，我们得到以下矩阵：

|  | **作为** | **配合** | **A** | **小提琴** | **你** | **类似** | **它** |
| **第 1 句** | 2. | 1. | 1. | 1. | 0 | 0 | 0 |
| **第 2 句** | 1. | 0 | 0 | 0 | 1. | 1. | 1. |

仅上述数据无法使我们准确预测新文档或文章所属的类别。这张桌子有一些固有的缺陷。在某些情况下，许多帖子中出现的较长文档或单词可能会降低算法的精度。可以删除停止字，以确保只分析相关数据。停止词包括 is、are、was 等。由于 bag of words 模型没有将语法因素纳入其分析，因此可以安全地删除停止词。也可以在停止语列表中添加一些你认为应该被排除在最终分析之外的词。

要生成进入矩阵列的值，我们必须标记培训数据：

```
    from sklearn.feature_extraction.text import CountVectorizer 
    from sklearn.feature_extraction.text import TfidfTransformer 
    from sklearn.naive_bayes import MultinomialNB 
    count_vect = CountVectorizer() 
    training_matrix = count_vect.fit_transform(training_data.data) 
```

`training_matrix`的尺寸为（225735788）。这意味着 2257 对应于数据集，而 35788 对应于构成所有帖子中唯一单词集的列数。

我们实例化`CountVectorizer`类并将`training_data.data`传递给`count_vect`对象的`fit_transform`方法。结果存储在`training_matrix`中。`training_matrix`包含所有独特的单词及其各自的频率。

为了缓解仅基于频率计数的预测问题，我们将导入有助于消除数据不准确的`TfidfTransformer`：

```
    matrix_transformer = TfidfTransformer() 
    tfidf_data = matrix_transformer.fit_transform(training_matrix) 

    print(tfidf_data[1:4].todense()) 
```

`tfidf_data[1:4].todense()`仅显示一个三行乘 35788 列矩阵的截断列表。所看到的值是术语频率——反向文档频率，用于减少使用频率计数导致的不准确度：

```
    model = MultinomialNB().fit(tfidf_data, training_data.target) 
```

`MultinomialNB`是朴素贝叶斯模型的一个变体。我们将合理化的数据矩阵`tfidf_data`和类别`training_data.target`传递给其`fit`方法。

# 预言

为了测试我们的模型是否掌握了足够的知识来预测未知帖子可能属于的类别，我们有以下样本数据：

```
    test_data = ["My God is good", "Arm chip set will rival intel"] 
    test_counts = count_vect.transform(test_data) 
    new_tfidf = matrix_transformer.transform(test_counts) 
```

列表`test_data`被传递给`count_vect.transform`函数，以获得测试数据的矢量化形式。为了获得术语频率——测试数据集的逆文档频率表示，我们调用`matrix_transformer`对象的`transform`方法。

为了预测文档可能属于哪一类，我们执行以下操作：

```
    prediction = model.predict(new_tfidf)  
```

循环用于迭代预测，显示预测它们所属的类别：

```
    for doc, category in zip(test_data, prediction): 
        print('%r => %s' % (doc, training_data.target_names[category])) 
```

当循环运行到完成时，将显示短语及其可能属于的类别。示例输出如下所示：

```
'My God is good' => soc.religion.christian
'Arm chip set will rival intel' => comp.graphics
```

到目前为止，我们所看到的是监督学习的一个主要例子。我们首先加载类别已知的帖子。然后将这些帖子输入最适合基于朴素贝叶斯定理的文本处理的机器学习算法。向模型提供了一组测试柱片段，并对类别进行了预测。

为了探索一个无监督学习算法的例子，我们将研究一些数据聚类的 k-means 算法。

# 无监督学习示例

一类学习算法能够发现可能存在于一组数据中的固有组。这些算法的一个例子是 k-means 算法。

# K-均值算法

k-means 算法使用给定数据集中的平均点来聚类和发现数据集中的组。K 是我们希望发现的集群数量。在 k-means 算法生成分组后，我们可以为它传递额外但未知的数据，以预测它将属于哪个组。

请注意，在这种算法中，只有未分类的原始数据被提供给算法。由算法确定数据中是否存在固有的组。

为了了解该算法的工作原理，我们将检查由 x 和 y 值组成的 100 个数据点。我们将把这些值提供给学习算法，并期望该算法将数据分为两组。我们将为这两个集合着色，以使簇可见。

让我们创建一个样本数据，包含 100 条*x*和*y*对记录：

```
    import numpy as np 
    import matplotlib.pyplot as plt 
    original_set = -2 * np.random.rand(100, 2) 
    second_set = 1 + 2 * np.random.rand(50, 2) 
    original_set[50: 100, :] = second_set 
```

首先，我们使用`-2 * np.random.rand(100, 2)`创建 100 条记录。在每个记录中，我们将使用其中的数据来表示最终将绘制的 x 和 y 值。

`original_set`中的最后 50 个数字将替换为`1 + 2 * np.random.rand(50, 2)`。实际上，我们所做的是创建两个数据子集，其中一个集合的数字为负数，而另一个集合的数字为正数。现在算法的责任是适当地发现这些片段。

我们实例化`KMeans`算法类并传递`n_clusters=2`。这使得该算法只将其所有数据分为两组。通过一系列的尝试和错误，得到了这个数字`2`。但出于学术目的，我们已经知道了这个数字。在处理现实世界中不熟悉的数据集时，这一点并不明显：

```
    from sklearn.cluster import KMeans 
    kmean = KMeans(n_clusters=2) 

    kmean.fit(original_set) 

    print(kmean.cluster_centers_) 

    print(kmean.labels_) 
```

将数据集传递给`kmean``kmean.fit(original_set)`的`fit`函数。该算法生成的聚类将围绕某个平均点旋转。定义这两个平均点的点由`kmean.cluster_centers_`获得。

打印时的平均点如下所示：

```
[[ 2.03838197  2.06567568]
 [-0.89358725 -0.84121101]]
```

`original_set`中的每个数据点在我们的 k-均值算法完成训练后将属于一个集群。k-mean 算法将发现的两个集群表示为 1 和 0。如果我们要求算法将数据分为四类，那么这些类的内部表示将是 0、1、2 和 3。要打印出每个数据集所属的各种集群，我们执行以下操作：

```
    print(kmean.labels_) 
```

这将提供以下输出：

```
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
```

有 100 个 1 和 0。每个显示每个数据点所属的集群。通过使用`matplotlib.pyplot`，我们可以绘制每个组的点，并将其适当着色，以显示集群：

```
    import matplotlib.pyplot as plt 
    for i in set(kmean.labels_): 
        index = kmean.labels_ == i 
        plt.plot(original_set[index,0], original_set[index,1], 'o') 
```

`index = kmean.labels_ == i`是一种巧妙的方法，通过它我们可以选择与组`i`对应的所有点。当`i=0`时，属于组 0 的所有点都返回索引。`index =1, 2`也一样，等等

`plt.plot(original_set[index,0], original_set[index,1], 'o')`然后使用 o 作为绘制每个点的字符绘制这些数据点。

下一步，我们将绘制形成簇的质心或平均值：

```
    plt.plot(kmean.cluster_centers_[0][0],kmean.cluster_centers_[0][1], 
             '*', c='r', ms=10) 
    plt.plot(kmean.cluster_centers_[1][0],kmean.cluster_centers_[1][1], 
             '*', c='r', ms=10) 
```

最后，我们用星号表示的两种方法显示整个图：

```
    plt.show()
```

![](assets/c8344882-002c-4e16-9613-0ef49cb21bc5.png)

该算法在样本数据中发现两个不同的聚类。两个星团的两个平均点用红星符号表示。

# 预言

通过我们获得的两个聚类，我们可以预测一组新数据可能属于哪个组。

让我们预测点`[[-1.4, -1.4]]`和`[[2.5, 2.5]]`将属于哪一组：

```
    sample = np.array([[-1.4, -1.4]]) 
    print(kmean.predict(sample)) 

    another_sample = np.array([[2.5, 2.5]]) 
    print(kmean.predict(another_sample)) 
```

输出如下所示：

```
[1]
[0] 
```

至少，我们可以预期这两个测试数据集属于不同的集群。当`print`语句打印 1 和 0 时，我们的期望被证明是正确的，从而确认我们的测试数据确实属于两个不同的集群。

# 数据可视化

数值分析有时不容易理解。事实上，一个图像值 1000 个单词，在本节中，一个图像值 1000 个仅由数字组成的表格。图像提供了一种快速分析数据的方法。大小和长度的差异是图像中可以得出结论的快速标记。在本节中，我们将介绍表示数据的不同方法。除了这里列出的图表，在聊天数据时还可以实现更多功能。

# 条形图

为了将值 25、5、150 和 100 绘制成条形图，我们将把这些值存储在一个数组中，并将其传递给`bar`函数。图中的条形表示沿*y*轴的幅值：

```
    import matplotlib.pyplot as plt 

    data = [25., 5., 150., 100.] 
    x_values = range(len(data)) 
    plt.bar(x_values, data) 

    plt.show() 
```

`x_values`存储`range(len(data))`生成的值数组。此外，`x_values`将确定*x*轴上绘制钢筋的点。第一根钢筋将在*x*轴上绘制，其中 x 为 0。带有数据 5 的第二个条形图将绘制在*x*轴上，其中 x 为 1：

![](assets/94bb2e81-77ba-4f7d-aa36-e9ac864e7330.png)

可以通过修改以下行来更改每个条的宽度：

```
    plt.bar(x_values, data, width=1.)  
```

这将生成以下图表：

![](assets/ebb857b3-a37f-4e0d-95c8-708daac185c1.png)

然而，这在视觉上并不吸引人，因为酒吧之间已经没有空间了，这使得它看起来很笨拙。现在，每个条在*x*轴上占据一个单位。

# 多重条形图

在尝试可视化数据时，通过堆叠多个条形图，可以进一步了解一段数据或变量如何随另一段数据或变量而变化：

```
    data = [ 
            [8., 57., 22., 10.], 
            [16., 7., 32., 40.],
           ] 

    import numpy as np 
    x_values = np.arange(4) 
    plt.bar(x_values + 0.00, data[0], color='r', width=0.30) 
    plt.bar(x_values + 0.30, data[1], color='y', width=0.30) 

    plt.show() 
```

第一批数据的 y 值为`[8., 57., 22., 10.]`。第二批为`[16., 7., 32., 40.]`。绘制条形图时，8 和 16 将并排占据相同的 x 位置。

`x_values = np.arange(4)`生成值为`[0, 1, 2, 3]`的数组。第一组钢筋首先在位置`x_values + 0.30`处绘制。因此，第一个 x 值将绘制在`0.00, 1.00, 2.00 and 3.00`处。

第二批`x_values`将绘制在`0.30, 1.30, 2.30 and 3.30`处：

![](assets/41a0d219-8d89-4ff1-8fd4-0e6f5cfe7162.png)

# 方框图

方框图用于显示分布的中值和高低范围。它也被称为“框须图”。

让我们绘制一个简单的方框图。

我们从正态分布生成 50 个数字开始。然后将其传递到`plt.boxplot(data)`以绘制图表：

```
    import numpy as np 
    import matplotlib.pyplot as plt 

    data = np.random.randn(50) 

    plt.boxplot(data) 
    plt.show() 
```

下图是所产生的结果：

![](assets/b5bdd866-0c2b-4e38-80f3-e0aeab3bbedf.png)

对上图的一些评论：方框图的特征包括一个跨越四分位区间的方框，用于测量离散度；数据的外边缘由连接到中心盒的晶须表示；红线代表中间值。

方框图有助于轻松识别数据集中的异常值，以及确定数据集可能向哪个方向倾斜。

# 饼图

饼图解释和直观地显示数据，就像将数据放入一个圆圈中一样。单个数据点表示为一个圆的扇形，其总和为 360 度。此图表也适用于显示分类数据和摘要：

```
    import matplotlib.pyplot as plt 
    data = [500, 200, 250] 

    labels = ["Agriculture", "Aide", "News"] 

    plt.pie(data, labels=labels,autopct='%1.1f%%') 
    plt.show() 
```

图形中的扇区使用标签数组中的字符串进行标记：

![](assets/90f4e80e-e057-4002-b122-5c82b2ea3684.png)